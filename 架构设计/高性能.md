# 高性# 高性能架构

## 前端框架

前端基于自研框架+Vue开发，具有以下优势：

- 减少http请求: 合并小图标、小css，启用浏览器缓存，缓存前端资源，使用了合理的代码拆分方案，按需加载脚本资源，从而减少请求资源的次数。
- 降低请求资源大小: 合理利用缓存,压缩静态资源，减少请求资源的体积，使用稳定算法预构建前端资源，减少前端资源变化频率，更有效利用浏览器缓存。
- 虚拟DOM: Vue.js使用虚拟DOM来优化DOM操作。通过在内存中维护一个虚拟DOM树，Vue.js可以比直接操作DOM更快地更新视图。
- 组件化: Vue.js的组件化开发模式使得开发者可以将复杂的应用程序拆分为多个小组件，并且每个组件都可以独立开发和测试。这种模式使得代码更加可维护，也使得性能更容易优化。
- 响应式数据绑定: Vue.js的响应式数据绑定使得数据的变化可以自动更新视图。开发人员可以专注于数据和业务逻辑，而无需手动管理DOM更新，从而提高开发效率和性能。
- 异步渲染: Vue.js的异步渲染机制可以提高页面的响应速度。通过异步渲染，Vue.js可以在渲染大量数据时避免UI阻塞，从而提高性能。

## 负载均衡

负载均衡其目的是为了提高系统的可用性、可扩展性、性能和稳定性。在Kubernetes集群中以下组件可以有效帮助系统实现负载均衡。

- Service: 在Kubernetes集群中业务服务运行在pod中，并且支持多实例运行，可有效规避单点故障问题。同时Kubernetes中的Service作为一种抽象的服务对象，可用于为一组具有相同功能的Pod提供统一的入口地址，当业务服务进行更新或重启时对于用户来说是无感知的。
- kube-Proxy: kube-proxy是集群中每个节点上所运行的网络代理， 是实现 Service的一部分。可以为Service对象提供负载均衡的功能，当Kubernetes集群中的Pod通过Service对象对外提供服务时，kube-proxy会根据Service对象的配置，将请求转发给对应的Pod。
- Ingress-Nginx: Ingress-nginx 可以将 Kubernetes 集群中的多个应用程序暴露到外部并提供负载均衡、流量路由和安全性控制等服务。能够根据请求的路径、主机名等信息，将流量路由到不同的后端服务上，能终止 SSL 连接，解密流量并将其转发到后端服务，从而减轻后端服务的负载。从而能够处理高并发的网络流量，以实现高可用性和可伸缩性，并为 Kubernetes 应用程序提供一个统一的入口点。

## 高性能组件

部分系统服务组件使用Rust开发，Rust语言具有以下优势

- 内存安全性: Rust通过强制内存安全性来避免内存泄漏和缓冲区溢出等错误。这种安全性是通过一些语言特性来实现的，如所有权系统和借用检查器，这些特性使得Rust能够在编译时捕获内存错误，而不是在运行时才发现。
- 零成本抽象: Rust提供了抽象的能力，但并不会因此引入额外的开销，因为Rust的设计者们致力于在不影响性能的情况下提供高级抽象。
- 零成本异常处理: 与其他语言相比，Rust的异常处理机制具有更小的性能开销，因为它使用的是零成本异常处理，这意味着只有在发生异常时才会生成代码，而且异常处理的过程与正常代码的执行路径非常相似。
- 零开销的抽象类型: Rust提供了抽象类型和泛型类型的支持，但是这些类型的实例化和使用并不需要运行时开销。
- 零成本的线程安全: Rust的线程安全性是通过编译时检查来实现的，而不是在运行时动态检查。这意味着Rust可以提供非常高效的线程安全性，而不会给应用程序带来额外的性能开销
- 支持并发编程: Rust的并发模型是基于“Actors”模式的，它允许在不同的线程之间安全地共享数据，而无需使用锁或其他同步机制

## 数据缓存

Redis是一个高性能的内存数据库，它以其快速的读写速度、持久化能力和可扩展性而闻名。在平台中，我们使用Redis作为数据缓存，以提高数据的读取速度和降低数据库的访问压力。

### 通过redis提供性能

使用Redis加速数据库读写，以提高系统的性能和响应速度。

- 缓存热数据：将数据库中经常访问和修改的数据存储到Redis中，以便在下一次访问时快速响应。可以通过Redis的GET和SET命令，将数据库中的数据存储到Redis中，并通过EXPIRE命令设置过期时间，以便在过期后自动删除。
- 使用Redis作为队列：将数据库中需要进行批量处理或异步处理的操作，放到Redis队列中，并通过Redis的LPUSH和RPOP命令，实现队列的读写操作。可以通过设置多个消费者来并发地处理队列中的数据，以提高系统的并发能力和响应速度。
- 使用Redis作为分布式锁：在多个应用服务器访问同一个数据库时，可以使用Redis作为分布式锁，以避免多个服务器同时修改同一个数据导致的冲突和错误。可以通过Redis的SETNX命令和EXPIRE命令，实现分布式锁的功能。
- 使用Redis作为计数器：在需要对某些数据进行计数时，可以使用Redis作为计数器，以提高系统的性能和可扩展性。可以通过Redis的INCR和DECR命令，实现计数器的加减操作，并通过GET命令，获取当前的计数值。
- 使用Redis作为数据预处理：在需要对数据库中的数据进行一些预处理时，可以使用Redis作为中间缓存，将处理结果缓存到Redis中，并在下一次访问时直接从Redis中获取，以提高系统的性能和响应速度。可以使用Redis的GET和SET命令，将预处理结果缓存到Redis中。

## 部署模式

Redis是一种高性能、高可用、持久化的内存数据库，它可以支持多种部署架构，包括以下几种：

- 单机部署
- 主从复制部署
- 哨兵部署
- 集群部署

平台使用集群部署模式，在集群中可以部署若干个主节点以及从节点，实现了一下特性：

- 高可用性：Redis集群可以使用主从复制和故障转移等机制，保证系统的高可用性。当主节点失效时，集群可以自动将从节点升级为主节点，从而避免数据丢失和服务中断。
- 高可靠性：Redis集群可以使用多副本机制，将数据复制到多个节点，从而保证数据的可靠性和持久性。即使一个节点失效，系统依然可以保证数据的可用性和一致性。
- 高扩展性：Redis集群可以方便地扩展节点数量，从而提高系统的处理能力和性能。可以通过添加新节点、水平扩展或垂直扩展等方式来满足系统的不断增长的需求。
- 负载均衡：Redis集群可以使用哈希槽分片机制，将数据分散到多个节点上，从而实现负载均衡和并行处理。可以根据系统的性能需求和数据分布情况来调整哈希槽的数量和分配策略。
- 高性能：Redis集群可以使用多个节点并行处理读写请求，从而提高系统的处理能力和性能。可以使用客户端分片机制来实现并行处理和高并发访问。

## 消息队列

Kafka是一个分布式、高可用、高吞吐量的消息队列系统。

### 使用kafka提高系统吞吐

平台中存在大量的数据流转，需要中间件保障数据高效稳定的数据传输。Kafka作为一种高性能、高吞吐量的分布式消息系统，具有以下特点：

- 高并发：Kafka可以支持数千个并发连接，每秒可以处理数百万条消息。
- 低延迟：Kafka的消息处理延迟通常在毫秒级别，可以实现实时的数据传输和处理。
- 分布式架构：Kafka的分布式架构可以将负载分散到多个节点上，以实现横向扩展和高可用性。
- 可靠性：Kafka采用复制机制保证数据的可靠性，即使出现节点故障也能够保证数据不丢失。

### kafka的部署模式

平台选用Kafka集群模式，Kafka集群由多个Broker节点组成，每个节点负责处理一部分的消息，同时也会维护一份完整的消息副本，以保证数据的可靠性和容错性。Kafka集群通常由多个Zookeeper节点协调和管理，Zookeeper节点存储了集群的元数据和配置信息，以及负责进行节点的选举和状态的监控。集群模式具有以下优点：

- 提高可靠性：在集群模式下，Kafka可以通过数据的复制和备份来提高数据的可靠性和容错性。每个分区都有多个副本，可以保证在一个节点出现故障的情况下，数据仍然可以被恢复。
- 提高吞吐量：Kafka集群可以通过添加Broker节点来实现横向扩展，从而提高吞吐量和性能。
- 实现负载均衡：在集群模式下，Kafka可以将消息负载分散到多个Broker节点上，从而实现负载均衡和高可用性。当某个节点出现故障时，消息可以被自动转移到其他节点上。
- 可扩展性：Kafka集群可以通过添加或删除Broker节点来实现动态扩展和缩减，从而适应不同的负载需求。
- 提高灵活性：Kafka集群可以支持多种不同的数据源和数据格式，可以满足各种不同的数据传输和处理需求。
- 提高安全性：Kafka集群可以实现安全的数据传输和访问控制，从而保护数据的安全性和隐私性。

## 资源调度

平台中将大量微服务或中间件打包为docker镜像，以容器的形式运行，平台需要一个有力的资源调度方案解决各类问题。

平台选用k8s作为容器编排与资源调度的基础。Kubernetes（k8s）是一个用于自动化容器化应用程序部署、扩展和管理的开源平台。它由Google开发，目前由Cloud Native Computing Foundation（CNCF）维护。Kubernetes可以在不同的云提供商和本地数据中心上运行，并支持多种容器引擎，例如Docker和rkt。Kubernetes提供了一组API和工具，帮助用户在容器化应用程序中自动处理负载均衡、自动扩展、滚动更新和故障恢复等任务。

## k8s的性能优势

- 自动负载均衡：Kubernetes可以自动将负载均衡到不同的容器实例中，从而实现应用程序的高可用性和可伸缩性。
- 自动扩展：Kubernetes可以自动扩展应用程序容器以满足流量需求，并在负载减轻时自动缩小容器实例。
- 高可用性：Kubernetes可以自动在多个节点之间重新分配容器实例，以确保应用程序的高可用性和可靠性。
- 故障转移：Kubernetes可以自动检测容器实例的故障，并自动重启或替换它们。
- 滚动更新：Kubernetes可以在不中断服务的情况下，自动地将应用程序容器实例进行滚动更新。
- 资源管理：Kubernetes可以自动管理资源（CPU、内存等），并确保容器实例获得足够的资源来运行应用程序。
- 容器隔离：Kubernetes可以在同一节点上隔离容器实例，以确保它们互不干扰。

## 图数据库

部分模块中需要维护复杂的拓扑关系，如果使用常规的关系型数据库会大幅提升开发成本以及系统IO。故而选择RedisGraph图数据库维护拓扑关系。RedisGraph是一个基于Redis的内存图形数据库，它能够高效地处理复杂的图形结构，并提供快速的查询和数据处理能力。RedisGraph的性能优势主要表现在以下几个方面：

- 高效的内存使用：RedisGraph采用了紧凑的内存存储结构，能够在使用较少内存的情况下存储大规模图形数据。
- 快速的查询速度：RedisGraph使用了基于Redis的高效内存访问方式，结合图形数据库的查询算法，能够在毫秒级别内快速处理大规模的图形数据。
- 可扩展的架构：RedisGraph采用分布式架构，能够方便地进行横向扩展，以处理更大规模的图形数据。
- 支持多种查询方式：RedisGraph支持多种查询方式，包括Cypher和GraphQL等，用户可以根据自己的需求选择适合的查询方式。
- 灵活的数据模型：RedisGraph支持灵活的数据模型，能够处理各种复杂的图形结构，包括有向图、无向图、属性图等等。

## 列式存储

平台中存在的大量的数据分析场景，包括但不限于：

- 大量的数据分析和统计任务，并存在非常复杂的查询和聚合操作。
- 实时查询
- 海量数据

平台引入clickhouse作为列式数据库解决以上问题。clickhouse是一个高性能的列式数据库，擅长解决以下场景的问题：

- 分析型场景：clickhouse适合处理大量的数据分析和统计任务，可以支持非常复杂的查询和聚合操作。
- 时序型场景：猛对于时序数据的处理非常高效，可以快速地处理时间序列数据，并且支持高度压缩和存储优化。
- 实时查询场景：clickhouse可以在毫秒级别内响应实时查询请求，适用于需要快速获取实时数据的场景。
- 海量数据场景：clickhouse可以支持PB级别的海量数据存储和处理，适用于大数据场景。

clickhouse的性能优势主要体现在以下几个方面：

- 高并发能力：clickhouse可以支持数百个并发查询，且查询响应时间非常短。
- 高性能计算：clickhouse采用了独特的内存映射技术和向量化计算方式，能够快速地进行复杂的聚合和计算操作。
- 数据压缩：clickhouse可以对数据进行多种不同的压缩方式，从而节省存储空间和提高查询速度。
- 分布式架构：clickhouse的分布式架构可以支持横向扩展和负载均衡，从而提高系统的可靠性和性能。
- 灵活的架构：clickhouse的设计非常灵活，可以支持多种不同的数据源和数据格式，能够适应各种不同的数据处理需求。

平台还对clickhouse的索引进行优化，索引（Index）是一种高性能索引技术。它基于区间分割和编码方式存储数据，能够大幅度提升索引查询的性能。 传统的B+树索引会将索引数据按照固定大小的块进行切分，然后将每个块存储到磁盘上，每次查询需要读取磁盘上的多个块进行查找，效率比较低。xx索引则将索引数据按照变长的区间进行划分，每个区间内的数据进行编码后存储在一个块中，每次查询只需要读取一个块，从而极大地提高了查询性能。xx索引还支持多维索引、压缩和快速跳过无用块等功能，能够适用于各种不同的数据类型和查询场景，是一种非常高效的索引技术。

## 资源控制

平台中有部分组件直接安装在物理机上，需要对其进行资源控制，防止其使用过多资源。 cgroup是Linux内核中的一种机制，它能够限制和监控进程的资源使用，例如CPU、内存、磁盘I/O等等。systemd通过cgroup机制，能够将进程组织成一个层次结构，并为每个层次结构分配一定数量的资源，从而实现对系统资源的控制和管理。systemd使用cgroup来限制各个服务的资源使用，例如CPU、内存、磁盘I/O等等。在systemd中，每个服务都有自己的cgroup，由systemd在启动服务时为其创建，并将服务的进程加入该cgroup中。然后，systemd可以使用cgroup的接口来控制和监控服务的资源使用，例如限制CPU使用率、设置内存限制、控制磁盘I/O等等。平台使用systemd在不同组件的service文件中预设资源限制，它能够将进程组织成一个层次结构，并为每个层次结构分配一定数量的资源，从而实现对系统资源的控制和管理，保证单个组件的资源占用不会影响其余组件。

cgroup的优势有以下几点：

- 提高系统的稳定性和可靠性：通过限制进程的资源使用，可以避免某个进程占用过多资源导致系统出现性能问题。
- 更好地控制系统资源：对于具有多个服务的系统而言，cgroup可以使我们更好地控制资源的使用，提高系统的可扩展性和可管理性。
- 提高系统的安全性：通过限制进程的资源使用，可以避免恶意代码或攻击者占用过多资源导致系统崩溃或拒绝服务。
- 更好地利用系统资源：通过使用cgroup，可以更好地利用系统资源，提高系统的利用率。

## 系统内核

平台的轻量化版本对内核参数或版本进行一定程度的修改。

### 实时 Linux

Linux 内核本质是一种分时操作系统，通过将 CPU 按照时间片进行划分，根据内核中 CFS 调度器（完全公平调度器) 计算的结果，对进程使用的时间片占用进行调度。这个在单机 A57 上存在一些问题：

- 在进行重 I/O 负载操作时， CFS 会更倾向于优先将资源调度给重 I/O 的程序或者内核 I/O 轮询，但是实际上 I/O 操作可能不会这么快完成，因此容易导致重计算响应的进程（例如 `Flink`、`Zookeeper`）没有足够的时间片进行调度
- 在类似 Flink 的任务进行重计算操作时，因为 cgroup 限制，CFS 调度器会认为这部分进程并非是优先级高的进程，因此对对应进程的响应优先级低，不会及时响应计算任务的资源请求，导致计算任务的进程的相对饥饿
- 内核频率为 250Hz，会导致一次不正常的资源分配，带来的时间片浪费更多

这些调度问题也是 CFS 调度器的通病。通常情况下，解决 CFS 调度问题有几种方向：

- 使用其他调度器，取代 CFS 。例如 `Project C PDS/BMQ` 调度器、 `Cacule` 调度器、`MuQSS` 调度器等
- 例如 Golang 等语言的程序，不使用 CFS 进行进程的调度，而是使用协程，自行对 CPU 资源进行管理调度
- 通过去掉内核中的自旋锁，实现内核实时化，支持中断抢占以及实时进程。

> 实时内核与分时内核最大的不同是强调响应时间，中断和资源请求必须在规定时间内得到响应

### BBR 与 BBRv2

[BBR](https://github.com/google/bbr) 是 Google 公司开源的 TCP 拥塞控制算法，在 linux 4.9 合并入树内主线分支。

对于 Linux 内核而言，一个完整的拥塞控制算法应该包括几部分：

- 慢启动算法
- 拥塞避免算法
- 快速重传算法
- 快速恢复算法

目前，在不进行默认调整的情况下，内核默认使用 `CUBIC` 进行拥塞控制。 `CUBIC` 是一种基于包丢失检测的算法，核心思想是一旦网络中发生的丢包，则说明网络上出现了流量拥塞，这个时候就需要将滑动窗口减半进入快重传、快恢复阶段。在实际网络中，由于长传输导致的概率丢包或者由于硬件性能导致的帧延迟，都有可能被 `CUBIC` 识别出现问题，影响网络传输效率；`BBR` 算法与 `CUBIC` 不同，是一种基于带宽检测的算法，核心思想是通过实时获取当前网络的最大带宽以及最小 RTT 时间，尽可能的填充 TCP 窗口接近最大实际链路带宽，进行拥塞控制。这种算法最大的差异就是，`BBR` 不受网络内可能的随机丢包影响，随机丢包不会导致链路速度的大幅度下降，进而充分提高 TCP 实际带宽表现

当前 Linux 内核主线分支中的 BBR 算法是 Google BBR 的 1.0 版本。这个版本 BBR 的问题在于两个方面：

- BBR 算法并不是一个基于丢包的公平竞争算法。当网络中存在基于 `CUBIC` 算法进行拥塞控制的机器较多时，BBR 算法的带宽探测会出现异常，导致错误的带宽评价，影响整体性能
- BBR 算法在整体网络拥塞的场景下，由于不受实时丢包影响，会导致网络拥塞更加严重；同时，BBR 算法必须使用更多的流量 `inflight` 流进行网络带宽探测，会导致闲时带宽挤占

针对 BBR 算法的缺陷，Google 提出了 BBRv2 算法。当前，该算法没有进入 Linux 内核主线分支，仅有 Google 的树外 patch 维护。

BBRv2 与原始 BBR 算法基本相同，但是在两个层面上有改动：

- 网络实际丢包将被动的影响 BBR 算法对最大带宽值的评估，从而避免中心网络拥塞的出现
- BBRv2 在 `ProbeBW` 阶段引入了一个子状态机，优化带宽评估机制，增强评估准确性

### Liquorix Xanmod 等内核补丁

#### Xanmod 补丁

XanMod 补丁集是内核源码树外的一个相对独立的内核发行版。与原版内核相比，XanMod 内核具有以下差别

- 集成了来自 le9 的内存高压力下保护性文件映射
- 内核树内的 linux/net-next 优化移植
- Fsync 优化
- Google 的 LRU 内存替代算法
- TCP BBRv2
- 来自 Intel 的 Clear Linux 补丁集部分补丁
- 来自 Graysky 开发的针对 GCC 以及 Clang 的额外编译优化，支持针对平台的优化以及 O3 级别优化
- `Cacule` CPU 调度器，基于响应比调度
- 默认基于 GCC12 编译、基于指定平台优化

XanMod 内核带来的一个最大的好处是，在内存压力非常高的情况下，机器仍旧可以准确的响应部分中断，从而配合 OOM-Killer 杀死部分进程，避免主机崩溃；同时，基于 GCC12 的平台优化选项，可以有效的提高编译后内核的汇编指令运行效率，在配合其他补丁使用时有效提高性能

#### UKSM 补丁

UKSM 是针对内核 KSM 功能的增强补丁。KSM(Kernel Samepage Merging) 内存相同页面合并在默认情况下效率极低，UKSM 补丁则通过定制优化以及算法优化等方案，将内存合并速度大幅度增强

但是，因为最新的 UKSM 补丁中包含两个华为工程师 `@colo-ft` 的错误修复，这个修复会引起 UKSM 在合并 JDK 内存时内核崩溃，因此并未使用最新版本，而是 `@colo-ft` 提交代码前的最后一个版本

#### CFS 调度器优化补丁

这个是 Intel 的 Clear Linux 补丁集中部分 patch 的衍生补丁。CFS 调度器补丁主要用于减少 Linux 内核中 CFS 调度器的自旋锁，从而有效提高内核进程的调度效率。

## 覆盖式网络

覆盖式网络（Overlay Network）是指在物理网络上构建的一种虚拟网络，可以在不同的物理网络中建立逻辑上的网络连接。覆盖式网络在云计算和容器化部署中被广泛应用，可以为多个主机提供一个虚拟的网络层，实现跨主机的通信和数据传输。

覆盖式网络的主要特点包括：

- 虚拟化：覆盖式网络是一种虚拟化的网络，可以将物理网络的多个子网或主机连接起来，形成一个虚拟的网络层，从而实现跨主机的通信和数据传输。
- 独立性：覆盖式网络是独立于底层物理网络的，可以在任何物理网络上构建，因此可以很好地支持多云、跨数据中心等多样化的部署场景。
- 可扩展性：覆盖式网络可以很好地支持扩展性，可以随着业务的发展而增加节点，并根据需要随时调整网络拓扑结构。
- 安全性：覆盖式网络可以提供安全性保证，可以对网络流量进行加密和隔离，防止恶意攻击和数据泄露。

覆盖式网络技术主要是为了解决容器化应用在跨主机通信时面临的种种问题，如主机网络拓扑复杂、IP地址管理难度大、网络安全难以保障等。覆盖式网络的优势包括：

- 网络隔离性好：容器的网络隔离可以在虚拟网络层面实现，不会对宿主机的网络环境造成影响。
- IP管理简单：容器的IP地址可以在虚拟网络层面上自由分配，不会与宿主机的IP地址发生冲突。
- 灵活性高：容器可以在任意宿主机之间迁移，覆盖式网络可以自动感知容器的位置变化，并调整网络拓扑，从而保证容器之间的通信不会受到影响。
- 安全性好：容器之间的通信可以通过加密等手段进行保护，避免了在物理网络中容易遭受攻击的情况。
- 扩展性强：覆盖式网络可以通过不同的实现方式，支持多种不同的容器编排工具，如Kubernetes、Docker Swarm等，从而适应不同的场景需求。
能