# nebula

## 美团

### 需求

- 数据实时写入、失败自动重试、写入速度控制、读写一致性要求不高
- 导入数据要求每小时10亿级别

### 使用场景

- 智能推荐：美团业务相关场景，包括外卖店铺推荐、美食推荐。
- 代码依赖分析：RD 可以查到他所写的代码影响了哪些外部接口，并且能查看相关的调用路径。
- 服务治理：微服务之间调用关系实时写入图数据库里，以此数据为基础，实现服务链路治理和告警优化工作。

### 实现方式

1、 数据实时写入、失败自动重试、写入速度控制
引入 Kafka 组件，业务方在服务中通过 SDK 对图数据库进行写操作时，将`写操作写到 Kafka 队列里`，之后由该应用下的多个集群异步消费这个 Kafka 队列。

- `实现失败重试`：每条kafka消息中存储单条写操作，从而实现失败重试的目的。
- `控制数据写入速度`：过改变 Kafka 分片数、Agent 中消费 Kafka 线程数来变更和调整 Kafka 中操作的消费速度。

2、 导入数据要求每小时10亿级别

- `官方导入支持每小时10亿级别导入`：Nebula Graph 对外提供的批量数据导入接口的写入速率大概是每小时 10 亿级别，导入百亿数据大概要耗费 10 个小时，耗时过久，需要优化。
- `美团优化方式：`在 Spark 集群中直接生成图数据库底层文件 sst file，再借助 RocksDB 的 bulkload 功能直接 ingest 文件到图数据库，从而实现每小时10亿级别数据导入。

## 快手

### 需求

- 支持每天新增写入数据量在百亿级别，小时级写完,、写入时感知数据异常且不丢失数据。
- 支持数据的实时写入，支持的 QPS 在 10W 量级。
- 毫秒级的在线实时查询，需要支持的 QPS 在 5W 量级。
- 稳定性要求高，要求四个9，故障毫秒级恢复。

### 使用场景

安全情报的图数据建模, 利用图数据库可以将每一个层次的实体ID通过关联关系串联起来，形成一张立体层次的网，通过这张立体层次的网能够使企业快速掌握攻击者的攻击方式、作弊工具、团伙特征等较全貌的信息。

### 实现方式

1、 支持每天新增写入数据量在百亿级别，小时级写完,、写入时感知数据异常且不丢失数据
快手部署了在线与离线两套图数据库集群，两个集群的数据采用同步双写，在线集群承担在线 RPC 类的服务，离线集群承担 CASE 分析及 WEB 查询的服务，这两个集群互不影响。 同时集群的状态监控与动态配置下发模块是打通的，当某一个集群出现慢查询或发生故障时，通过动态配置下发模块来进行自动切换，做到上层业务无感知。

2、 稳定性要求高，要求四个9，故障毫秒级恢复
存储层进行代码改进与参数优化。其中包括：

- 优化 Raft 心跳逻辑
- 改进 leader选举和 log offset 的逻辑
- 对 Raft 参数进行调优等，来提升单集群的故障恢复时间

结合客户端重试机制的优化，使得 Nebula 引擎在用户体验上从最初的故障直接掉线改善为故障毫秒级恢复。

3、 对超级节点（出度达到百万/千万的点）慢查询的优化

- `limit截断`：查询中做符合条件的 limit 截断，实现查询时对边的裁剪。
- `边采样`：对于不支持limit截断的场景，在查询时，按一定比例进行边采样，实现根据采样信息对边进行裁剪。

4、 查询客户端的改造与优化

- **连接池化**：Nebula Graph  官方客户端未提供连接池，在高频查询场景中该问题极大地影响着系统的性能与稳定性。快手通过连接池化技术对官方客户端进行二次封装，并对连接生命周期的各个阶段进行监控，实现了连接的复用和共享，提升了业务稳定性。
- **自动故障切换**：通过对连接建立、初始化、查询、销毁各个阶段的异常监控和定期探活，实现了数据库集群中的故障节点的实时发现和自动剔除，如果整个集群不可用，则能秒级迁移至备用集群，降低了集群故障对在线业务可用性造成的潜在影响