# LLM

## 术语表

| 英文缩写 | 英文                        | 中文           |
| -------- | --------------------------- | -------------- |
| NLP      | Natural Language Processing | 自然语言处理   |
| PLM      | Pre-train Language Model    | 预训练语言模型 |
| NLM      | Netural Language Model      | 神经语言模型   |
| LLM      | Lagre Language Model        | 大语言模型     |

发展路径

- 统计语言模型: SLM, N元语言模型, 基于马尔科夫假设建立词预测模型
- 神经语言模型: NLM, RNN来描述单词序列的概率, Word2Vec提出构建一个简化浅层神经网络
- 预训练语言模型: PLM, 基于高度并行化Transformer架构, BERT, 确立了"预训练和微调"学习范式
- 大语言模型: LLM, 千亿参数的transformer语言模型，GPT, 拥有涌现、思维链等能力

## 涌现&思维链

- 伸缩法则: 参数规模的扩大，任务效果持续增长
- 涌现能力: 参数规模达到某个临界点，任务效果骤然向好

**上下文学习和思维链是人工智能解决实际业务问题的核心、必要的关键能力**

- 上下文学习: LLM处理复杂任务的核心能力。临界值为700亿
- 思维链等复杂推理能力: 具备了通过prompt一步步引导模型从而完成复杂推理任务的能力。临界值为5000亿参数

`LangChain`, `Auto-GPT`等项目，都是通过思维链的方式实现LLM自动拆解，逐步完成一个任务。 核心思路都是对LLM思维链进行工程化封装，使得大模型能够根据自己的分析结果模式化调用可以执行特定任务的工具，本质都是Prompt工程的系统化实现

## prompt工程

> 为AI模型创建输入以改进给特定任务的输出过程

提示(prompt)是触发AI模型生成内容的宽泛指令; 它可以是一条语句，一段代码或一串单词，收到提示或输入后，AI模型会产生输出作为响应，输出质量很大程度受提示的影响

- zero-shot
- one-shot
- few-shot

## 训练方法

- 知识预训练: 使用原始训练数据集，进行预训练
- 任务微调: 使用特定数据集增强训练，可以强化模型在特定领域的性能
- 反馈学习: 使用RLHF进行微调, PPO
  - 监督学习: 人工标注数据集，训练一个LM
  - 训练奖励模型: 收集人工标准的模型多个输出之间的排序数据集，并训练一个奖励模型
  - 基于强化学习持续迭代模型: 使用奖励模型作为奖励函数，以PPO的方式，微调初始模型
- 推理加速: Paged Attention

## 发展方向

从决策型AI，转变成生成式AI

- 检索结合， 改善实时性和实时性
- 多模态理解和生产
- 调用外部能力