# LLM

## 术语表

| 英文缩写     | 英文                                       | 中文                   |
| ------------ | ------------------------------------------ | ---------------------- |
| NLP          | Natural Language Processing                | 自然语言处理           |
| PLM          | Pre-train Language Model                   | 预训练语言模型         |
| NLM          | Netural Language Model                     | 神经语言模型           |
| LLM          | Lagre Language Model                       | 大语言模型             |
| ICL          | In-Context Learning                        | 上下文学习             |
| CoT          | Chain of Thought                           | 思维链                 |
| RLHF         | Reinforcement learning from Human Feedback | 基于人类反馈的强化学习 |
| GPT          | Generative Pre-Training                    | 生成式预训练           |
| Demostration | Demostration                               | 示范                   |

发展路径

- 统计语言模型: SLM, N元语言模型, 基于马尔科夫假设建立词预测模型
- 神经语言模型: NLM, RNN来描述单词序列的概率, Word2Vec提出构建一个简化浅层神经网络
- 预训练语言模型: PLM, 基于高度并行化Transformer架构, BERT, 确立了"预训练和微调"学习范式
- 大语言模型: LLM, 千亿参数的transformer语言模型，GPT, 拥有涌现、思维链等能力

## 原理

- 模型缩放：增加模型的参数规模以提高其结果表现的过程 
- 模型容量：模型能够学习的函数族的大小，该函数由模型的参数空间定义，增加模型的参数规模可以增加模型的容量，从而使其能够学习更复杂的函数

### 涌现&思维链

- 伸缩法则: 参数规模的扩大，任务效果持续增长
  - KM扩展法则:  神经语言模型的性能与模型规模、数据集规模和训练计算量之间的幂律关系
  - Chinchill扩展法则: 模型大小和数据大小应该以相同比例增加
- 涌现能力: 参数规模达到某个临界点，任务效果骤然向好
  - 上下文学习: 假设已经为语言模型提供了一个自然语言指令和/或几个任务演示，它可以通过完成输入文本的单词序列方式为测试实例生成预期的输入，而无需额外的训练或梯度更新。是LLM处理复杂任务的核心能力。不同模型和任务不一样，临界值为700亿
  - 指令遵循: 使用自然语言描述的混合多任务数据集进行微调, 在从未见过的以指令形式描述的任务上表现出色, 通过指令微调，LLM能够在没有使用显式示例的情况下遵循新的任务指令, 因此它具有更好的泛化能力
  - 逐步推理: 小型语言模型通常很难解决涉及多个推理步骤的复杂任务，例如数学问题。 通过使用思维链提示策略，LLM可以通过利用包含中间推理步骤的提示机制来解决这类问题。 这种能力可能是通过在代码上进行训练而获得, 例如使用代码数据进行训练的模型Codex, 在大量Github代码上微调的GPT模型，可以解决非常困难的编程问题, 并且在数据问题上有显著的性能提升
思维链等复杂推理能力: 具备了通过prompt一步步引导模型从而完成复杂推理任务的能力。临界值为5000亿参数

`LangChain`, `Auto-GPT`等项目，都是通过思维链的方式实现LLM自动拆解，逐步完成一个任务。 核心思路都是对LLM思维链进行工程化封装，使得大模型能够根据自己的分析结果模式化调用可以执行特定任务的工具，本质都是Prompt工程的系统化实现
**上下文学习和思维链是人工智能解决实际业务问题的核心、必要的关键能力**

### 关键技术

- 扩展: transformer语言模型存在明显的扩展效应: 更大的模型/数据规模和更多的训练计算通常会导致模型能力的提升。
- 训练: 分布式训练算法是学习LLM网络参数所必需的，其中通常联合使用各种并行策略。
  - 已经发布了一些优化框架来促进并行算法的实现和部署, 例如DeepSpeed 和 Megatron-LM
  - 优化技巧对于训练稳定性和模型性能也很重要，例如重新开始以客服训练损失激增和混合精度训练
- 能力引导: LLM具备了作为通用任务求解器的潜在能力，可以设计合适的任务指令或具体的ICL策略激发这些能力, 例如包含中间推理步骤，CoT提示已被证明对解决复杂的推理任务有效
- 对齐微调: Instruct GPT设计了一种有效的微调方法，使LLM能够按照期望的指令进行操作, 利用了基于人类反馈的强化学习技术, 应用强化学习（RL）来学习由人类标注的偏好比较工作
- 工具操作: 利用外部工具来弥补LLM的不足, 例如利用搜索引擎检索未知知识, ChatGPT已经实现

**每个NLP任务可以被视为基于世界文本的子集的单词预测问题。因此如果模型训练后具有足够能力以复原时间文本，无监督语言模型建模可以解决各种任务**

### prompt工程

> 为AI模型创建输入以改进给特定任务的输出过程

提示(prompt)是触发AI模型生成内容的宽泛指令; 它可以是一条语句，一段代码或一串单词，收到提示或输入后，AI模型会产生输出作为响应，输出质量很大程度受提示的影响

### 局限性

- 特定上下文中生成错误的幻觉
- 存在潜在风险的回应

## LLM资源

### 公开可用的模型

百亿参数量级模型, 通常需要数百甚至上千个GPU或TPU

- LLaMA: 最大版本650亿参数, 指令遵循相关的任务中表现了卓越的性能
- NLLB: 最大版本545亿参数
- Flan-T5: 最大110亿版本，作为研究指令微调的首选模型，增加任务数量、扩大模型规模和使用CoT提示数据进行微调
- CodeGen: 11B, 是一个为生成代码设计的自回归语言模型, 可用作探索代码生成能力的候选模型
- MT0: 13B，是一个多语言任务模型

千亿参数量级别模型, 通常需要数千个GPU和TPU进行训练

- OPT：175B专注于复现和开源
- OPT-IML: 进行了指令微调，是研究指令微调效果的较好选择

### 公开可用的API

GPT-3系列：

- ada: 在OpenAI的主机服务器上进一步微调
- babbage: GPT(1B)
- curie: GPT-3(6.7B)
- davinci: GPT-3系列最强大的版本, GPT-3(175B)
- text-ada-001:
- text-babbage-001:
- text-curie-001:
- text-davinci-002:
- text-davinci-003:
- gpt-3.5-turbo-0301:

GPT-4系列：

- gpt-4:
- gpt-4-0314:
- gpt-4-32k-0314:

Codex

- code-cushman-001:
- code-davinci-002

### 常用语料库

- Books:
  - BookCorpus: GPT常用的数据集，超过11000本电子书
  - Gutenberg,超过70000本文学作品
- Common Crawl:
  - C4
  - CC-Stories:
  - CC-News:
  - RealNews:
- Reddit Links:
  - WebText: 由Reddit高赞链接组成
  - OpenWebText:
  - PushShift.io:  实时更新的数据集
- Wikipedia: 在线百科全书
- Code:
  - Github:
  - StackOverFlow:
  - BigQuery:
- Others:
  - The Pile: 一个大规模、多样化、开源的文本数据集, 超过800GB数据

### 代码库资源

- Transformers: 使用transformer 架构构建模型的开源python库，由Huggin Face开发和维护, 定期更新和改进模型和算法
- DeepSpeed: 由Microsoft开发的深度学习优化库，用于训练多个LLM，例如MT-NLG和BLOOM
- Megatron-LM: 由NVIDIA开发的深度学习库，用于训练LLM，提供丰富的分布式训练优化技术，包括模型和数据并行，混合精度训练和Flash Attention，这些优化技术可以大大提高训练效率和速度
- JAX: 由Google开发的用于高性能机器学习算法的python库，允许使用GPU进行数组高效运算

## 训练方法

### 数据处理

质量过滤->去重->隐私去除->分词

### 数据训练

- 知识预训练: 使用原始训练数据集，进行预训练
- 任务微调: 使用特定数据集增强训练，可以强化模型在特定领域的性能
- 反馈学习: 使用RLHF进行微调, PPO
  - 监督学习: 人工标注数据集，训练一个LM
  - 训练奖励模型: 收集人工标准的模型多个输出之间的排序数据集，并训练一个奖励模型
  - 基于强化学习持续迭代模型: 使用奖励模型作为奖励函数，以PPO的方式，微调初始模型
- 推理加速: Paged Attention

架构

- 编码器-解码器架构: Encoder-decoder, 由两个Transformer块分别作为解码器和编码器
- 因果解码器架构: Causal decoder, 采用单向注意力掩码，以确保每个输入token只能关注过去的token和它本身
- 前缀解码器架构: Prefix decoder, 修正了因果解码器的掩码机制，以使其能够对前缀token执行双向注意力

Transformer配置：

- 标准化: DeepNorm, PreLayerNorm
- 位置编码: 绝对位置编码，正弦函数和学习的位置编码, 相对位置编码，根据键和查询之间偏移量生成嵌入, RoPE被广泛用于一些最新的LLM
- 激活函数: GeLU激活函数、GLU激活函数
- 注意力和偏置: 去除偏置可以增强训练的稳定性

建议选择前置的RMS进行层标准化，并进行SwiGLU或GeGLU作为激活函数。此外，在位置编码方面，RoPE或ALiBi是更好的选择，因为它们在长序列上表现更好

### 预训练任务

预训练在将大规模语料库中的通用知识编码到巨大的模型参数中起着关键作用, 有以下两个常用的预训练任务

- 语言建模: 语言建模任务(LM)是预训练仅包含解码器的LLM最常用的目标, 旨在基于序列中前面的token x(i-1), 自回归地预测目标token xi
- 去噪自编码: 采用DAE(去噪自编码任务)作为预训练目标的现有LLM包括T5和GLM。这些模型主要是通过自回归地恢复替换区间来进行训练

通过使用语言模型目标进行预训练，因果解码器架构似乎可以实现更优越的零样本和小样本泛化能力, 并且可以通过扩展模型大小，数据集大小和总计算量

#### 模型训练

优化设置

- 批量训练: 批量大小设置为较大数字(2048个例子或400万个token), 新的策略是在训练过程中动态增加批量大小
- 学习率: 包括预热和衰减， 先预热，后衰减
- 优化器: Adam优化器和AdamW优化器
- 稳定训练: 使用权重衰减(weight decay) 和梯度裁剪(gradient clipping)

可扩展的训练技术

- 3D并行
  - 数据并行: 将模型参数和优化器复制到多个GPU上, 然后将整个训练浴帘分配到这些GPU上
  - 流水线并行: 将LLM的不同层分配到多个GPU上, 流水线并行将连续的层加载到同一个GPU上
  - 张量并行: 多GPU加载二分解LLM, 专注于分解LLM的张量(参数矩阵)
- Zero: 专注于解决数据并行中的内存冗余问题: 包括优化器状态分区、梯度分区和参数分区等
- 混合精度训练: 利用16位浮点数，减少内存使用和通信开销, 比FP16分配更多的指数位，更少的有效位

### 适配微调

预训练后，LLM可以获得解决各种任务的通用能力。 但LLM能力可以进一步适配到特定的目标

#### 指令微调

instruction tuning: 在自然语言格式的实例集合上微调预训练后LLM的方法，旨在增强LLM的能力

- 增加指令: 随着任务数增加， 模型性能最初呈现连续增长的趋势，但任务数量达到一定水平时，模型性能的提成变得微不足道
- 设计格式: 任务描述是LLM理解任务的最关键部分，适当数量的示例作为示范，对模型可以产生实质性的改进，也减轻了对指令工程的敏感性, 指令多样性比示例数量更重要
- 平衡数据分布: 微调过程中平衡不同任务的比例，提高高质量数据集的采样比例，可以带来性能提升

效果

- 性能改进: 不同规模的模型都可以从指令微调中收益，与预训练相比，因为LLM所需的指令数量明显少于预训练数据，指令微调成本较低
- 任务泛化性: 鼓励模型理解用于完成任务的自然语言指令, 赋予LLM遵循人类指令执行特定任务的能力，即使在未见过的任务上也能够执行

#### 对齐微调

alignment tuning: 旨在将LLM的行为和人类的价值观或偏好对齐, 对齐微调可能在某种程度上损害LLM的通用能力

对齐的标准  

- 有效性: 能简明扼要且高效的方式，帮助用户解决任务或回答问题
- 诚实性: 向用户提供准确的内容，不会捏造信息
- 无害性: 要求模型生成的语言不得是冒犯性或歧视性的

基于人类反馈的强化学习  

强化学习(RL)算法例如近端策略优化(Proximal Police), 通过学习奖励模型LLM适配人类反馈, 这种方法将人类纳入训练的循环中来开发对齐良好的LLM，例如InstructGPT

- 要对齐的PLM
- 从人类反馈中学习的奖励模型
- 训练LM的RL算法: 例如PPO是一种现有工作中广泛使用RL对齐算法

#### 高效微调

- 适配器微调: 在Transformer模型中引入小型神经网络模块，将原始特征向量压缩到较小的维度，然后将其回复到原始维度
- 前缀微调: 在语言模型的每个Transformer层中添加一系列前缀，这些前缀是一组可训练的连续向量
- 提示微调: 在输入层中加入可训练的提示向量
- 低秩微调: 通过添加低秩约束来近似每层的更新矩阵

### 使用

使用LLM主要方法是解决各种任务设计适当的提测策略

- ICL: 将任务描述和示范以自然语言文本形式表达的上下文学习。 
  - 预训练如何影响上下文学习: 当训练数据可以被类聚成许多不常见的类别，而不是均匀分布时，会表现出ICL的能力
  - 大模型如何实现上下文学习: 从梯度下降的角度进行分析，并将ICL视为隐式微调
- COP: 将一系列中间推理步骤加入提示来增强ICL, 例如算术推理、常识推理和符号推理
  - few-shot: 小样本思维, self-consistency提出了一种在生产CoT和最终答案时新的解码策略。首先用LLM生成多个推理路径, 然后对所有答案进行集成, 多样化的推理路径是CoT推理性能提高的关键
  - zero-shot: Let's think step by step， therefore， the answer is 
  - CoT能力的来源归因于使用代码进行训练

示范格式

- Auto-CoT: 利用LLM使用零样本提示 "Let's think step by step"来生成中间推理步骤
- Least-to-most: 提示首先询问LLM来执行问题分解，然后利用LLM根据先前解决的中间答案依次解决子问题

模型专业化可以应用于解决各种任务，包括问答、代码生成和信息检索

### 评测

- 语言生成:
  - 语言建模:
  - 条件文本生成: 基于给定的条件生成满足特定任务需求的文本，包括机器翻译、文本摘要和问答系统， 在现有的数据集上取得了显著的性能
  - 代码合成: APPS、HumanEval和MBPP等，表现与人类想当
- 知识运用: 基于事实证据的支撑，完成知识密集型任务的重要能力
  - 闭卷问答: 从预训练语料库中习得事实知识，与最好的开放领域问答系统表现相匹配
  - 开卷问答: 可以从外部知识库或文档集合中提取有用证件，可以大大提升生成答案的准确性
  - 知识补全
- 复杂推理: 理解和利用相关证据或逻辑来推导结论或做出决策的能力
  - 知识推理: 依赖于逻辑关系和事实知识的证据来回答给定的问题, 由于知识推理任务的复杂性，在例如常识推理等任务上，当前LLM的性能仍然落后于人类的结果
  - 符号推理: 主要关注与形式化规则设定中，操作符号以实现某些特定目标
  - 数学推理: 数学推理任务需要综合利用数学知识、逻辑和计算来解决问题或生成证明的过程, 主要分为数学问题求解和自动定理证明

主要问题

- 可控生成: LLM可以很好地处理局部关系，但可能难以解决全局关系, 即长程相关性
- 专业化生成: 处理专业领域或任务时，生成能力仍然可能受到限制
- 幻觉: 生产的信息与现有来源相冲突(内在幻觉)，或无法通过现有来源验证（外在幻觉）
- 知识实时性: 需要使用比训练数据更新的知识的任务时，LLM在解决这些任务时会遇到困难
- 不一致性: 将复杂的推理任务重新形式化为代码生成任务, 而生成的代码会被严格执行，确保推理过程和结果之间的一致性
- 数值计算: 在数值计算仍面临困难，特别是对于预训练阶段很少遇到的符号，例如大数字的算术符号等, 可以通过外部工具来提高

### 高级能力

上述三种能力对于LLM在实际应用中表现出了巨大价值

- 与人类对齐
- 与外部环境交互
- 工具操作

## 总结与未来方向

- 理论与原理: 对于LLM的潜在工作机制，最大的谜题是其如何通过非常大且深的神经网络分配、组织和利用信息
- 模型架构: 堆叠的多头自注意力层组成的Transformer，由于其可扩展性和有效性，已成为构建LLM的基本架构
- 模型应用: 提示已经成为使用LLM的主要方法。通过将任务描述和示例合并到提示中，ICL赋予了LLM在新任务上表现良好的能力，甚至在某些情况下胜过全数据微调模型
- 安全与对齐: RLHF严重依赖专业标注者高质量人类反馈数据，使得在实践中难以适当实施
- 应用与生态: 随着LLM在解决各种任务方面表现出强大的能力，他们可以应用于广泛的现实世界应用中