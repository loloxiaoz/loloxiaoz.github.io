# 模型微调

## LLM的区别

| 模型            | 结构            | 位置编码      | 激活函数 | layer nom 方法  |
| --------------- | --------------- | ------------- | -------- | --------------- |
| 原生transformer | Encoder/Decoder | Sinusoida编码 | ReLU     | Post layer norm |
| BERT            | Encoder         | 绝对位置编码  | GeLU     | Post layer norm |
| LLaMA           | Casual decoder  | RoPE          | SwiGLU   | Pre RMS Norm    |
| ChatGLM-6B      | Prefix decoder  | RoPE          | GeGLU    | Post Deep Norm  |
| Bloom           | Casual decoder  | ALiBi         | GeLU     | Pre Layer Norm  |

## 方法

- Prefix-Tuning/Prompt-Tuning: 在模型的输入或隐层添加k个额外可训练的tokens, 只训练这些前置参数
- Adapter-Tuning: 将较小的神经网络层或模块插入预训练模型的每一层，下游任务微调时也只训练这些适配参数
- Lora: 通过学习小参数的低秩矩阵, 来近似模型权重矩阵W的参数更新, 训练时只优化低秩矩阵参数

## 幂律

随着`模型大小`, `数据集大小`和`训练强度`，模型性能会提高，并且为了获得最佳性能，所有三个因素必须同时放大, 当不受其他两个因素制约时，模型性能与每个单独的因素都有幂律关系