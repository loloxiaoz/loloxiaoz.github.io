# 模型微调

> `预训练语言模型` + `下游任务微调`成为主流范式

## 原理

- Self-Attention: 自注意力机制, 更容易捕获句子中长距离的相互依赖的特征, 在计算过程中直接将句子中任意两个单词通过一个计算步骤联系起来
- Multi-Head Attention: 多头注意力机制, 由多个自注意力组成
  - 扩展了模型专注于不同位置的能力  
  - 将输入Embedding 投影到不同的表示子空间, 多个head学习到的Attention侧重点可能略有不同, 给了模型更大容量

随着`模型大小`, `数据集大小`和`训练强度`，模型性能会提高，并且为了获得最佳性能，所有三个因素必须同时放大, 当不受其他两个因素制约时，模型性能与每个单独的因素都有幂律关系

## LLM的区别

| 模型            | 结构            | 位置编码      | 激活函数 | layer nom 方法  |
| --------------- | --------------- | ------------- | -------- | --------------- |
| 原生transformer | Encoder/Decoder | Sinusoida编码 | ReLU     | Post layer norm |
| BERT            | Encoder         | 绝对位置编码  | GeLU     | Post layer norm |
| LLaMA           | Casual decoder  | RoPE          | SwiGLU   | Pre RMS Norm    |
| ChatGLM-6B      | Prefix decoder  | RoPE          | GeGLU    | Post Deep Norm  |
| Bloom           | Casual decoder  | ALiBi         | GeLU     | Pre Layer Norm  |

## 参数高效微调

参数高效微调是指微调少量或额外的模型参数, 固定大部分预训练模型参数，从而降低计算和存储成本, 某些场景下比全量微调效果更好  
主要分为以下三类方法:

- 增加额外参数: Adapter-like、Soft Prompts
- 选取一部分参数更新: BitFit
- 引入重参数化: Lora

常见微调技术

- BitFit: 仅调整bias就能有不错的效果
- Prefix-Tuning: 在每一个Transformer层都带上一些virtual token作为前缀, 以适应不同的任务
- Prompt-Tuning: Prefix-Tuning的简化版本, 在模型的输入或隐层添加k个额外可训练的tokens, 只训练这些前置参数, 更节省内存
- P-Tunning: 将Prompt转换成可以学习的Embedding层（由双向LSTM+两层MLP组成), 来建模virtual token的相互依赖会收敛更快, 效果更好
- P-Tunning V2: 在每一个Transformer层都加入了prompt token作为输入, 引入多任务学习
  - 解决了Prompt Tuning无法在小模型上有效提升的问题  
  - 移除了对模型效果改进较小的重参数化编码器(如P-Tunnig中的LSTM)
  - 对于一些复杂的硬序列标记任务取得了不错的效果
- Adapter-Tuning: 将较小的神经网络层或模块插入预训练模型的每一层，下游任务微调时也只训练这些适配参数, 在推理时会额外增加推理时长
- LoRA: 通过学习小参数的低秩矩阵, 来近似模型权重矩阵W的参数更新, 训练时只优化低秩矩阵参数
  - 可以通过可插拔的形式切换到不同的任务
  - 设计的比较好，简单且效果好
- QLoRA: 将预训练模型量化为4 bit，然后添加一组可学习的低秩适配器权重

总结: 像P-Tunning v2、LoRA等都是综合评估很不错的高效微调技术, 如果显存资源有限,可以考虑QLoRA, 如果只是解决一些简单任务场景, 可以考虑P-Tunning、Prompt Tuning 

## 最佳实践

- 明确指出参数数量类型
- 使用不同大小的模型进行评估
- 和类似方法进行比较
- 标准化PEFT测量基准
- 重视代码清晰度, 以最小化进行实现